{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dia 4\n",
    "Como lidar com problemas não supervisionados e análise de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Para que server algoritmos não-supervisionados? E como posso medir seu desempenho?\n",
    "\n",
    "Exemplo do algoritmo K-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': [12, 20, 28, 18, 29, 33, 24, 45, 45, 52, 51, 52, 55, 53, 55, 61, 64, 69, 72],\n",
    "    'y': [39, 36, 30, 52, 54, 46, 55, 59, 63, 70, 66, 63, 58, 23, 14, 8, 19, 7, 24]\n",
    "})\n",
    "plt.scatter(df['x'], df['y'], color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(df)\n",
    "labels = kmeans.labels_\n",
    "print(labels)\n",
    "colormap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "plt.scatter(df['x'], df['y'],  c=labels.astype(np.float))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilização interessante para o K-means, vamos realizar um Vector Quantization de uma imagem do summer palace, reduzindo o número de cores unicas de 96,615 para 64, tentando preservar a aparencia e qualidade da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Carregando e mostrando a imagem original\n",
    "\n",
    "china = load_sample_image(\"china.jpg\")\n",
    "\n",
    "\n",
    "#  Conversão para float em vez dos 8-bit int, dividindo por 255, \n",
    "#é importante a conversão para float para a impressão correta da imagem \n",
    "# transformando a imagem em um np.array de 2 dimensões\n",
    "china = np.array(china, dtype=np.float64) / 255\n",
    "\n",
    "#print(china.shape)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Original image (96,615 colors)')\n",
    "plt.imshow(china)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#quantidade de cores e clusters\n",
    "k_colors = 64\n",
    "\n",
    "w, h, d = original_shape = tuple(china.shape)\n",
    "assert d == 3\n",
    "image_array = np.reshape(china, (w * h, d))\n",
    "\n",
    "image_array_sample = shuffle(image_array, random_state=0)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, agora que configuramos a imagem e escolhemos aleatoriamente um conjunto menor de dados, treine o kmeans criando um objeto com o nome \"kmeans\" e teste com a imagem completa em modo de np.array \"image_array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Acrescente o código aqui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codebook_random = shuffle(image_array, random_state=0)[:k_colors + 1]\n",
    "print(\"Predicting color indices on the full image (random)\")\n",
    "labels_random = pairwise_distances_argmin(codebook_random,\n",
    "                                          image_array,\n",
    "                                          axis=0)\n",
    "\n",
    "\n",
    "def recreate_image(codebook, labels, w, h):\n",
    "    \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
    "    d = codebook.shape[1]\n",
    "    image = np.zeros((w, h, d))\n",
    "    label_idx = 0\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            image[i][j] = codebook[labels[label_idx]]\n",
    "            label_idx += 1\n",
    "    return image\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, K-Means)')\n",
    "plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n",
    "\n",
    "plt.figure(3)\n",
    "plt.clf()\n",
    "ax = plt.axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.title('Quantized image (64 colors, Random)')\n",
    "plt.imshow(recreate_image(codebook_random, labels_random, w, h))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, iremos testar o k-means com uma base de dados já conhecida, a base de plantas iris, vamos tentar agrupar os dados já conhecidos. Crie um dicionário com 3 estimadores, variando o número de grupos e um com um início diferente para os centroides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coloque o dicionário aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção, substitua o comentário pelo treino do estimador e a variável \"labels\" pelos grupos criados por cada estimador. (Apenas para melhor visualização de dados) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fignum = 1\n",
    "for name, est in estimators.items():\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "    plt.cla()\n",
    "    \n",
    "    \n",
    "    #substitua aqui\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(4, 3))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean() + 1.5,\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering é uma familia de algoritmos de clustering que gera clusters interligados juntando ou separando sucessivamente os dados. O Hierachical algoritm é representado como uma árvore. A raiz é o unico cluster que junta todos os samples e as folhas são clusters de apenas um sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.utils.testing import SkipTest\n",
    "from sklearn.utils.fixes import sp_version\n",
    "\n",
    "face = misc.face(gray=True)\n",
    "\n",
    "# Redução da imagem para 10% visando diminuir o processamento\n",
    "face = misc.imresize(face, 0.10) / 255.\n",
    "\n",
    "X = np.reshape(face, (-1, 1))\n",
    "#tipo de conectividade entre clusters\n",
    "connectivity = grid_to_graph(*face.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#crescente o código aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = np.reshape(ward.labels_, face.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(face, cmap=plt.cm.gray)\n",
    "for l in range(n_clusters):\n",
    "    plt.contour(label == l, contours=1,\n",
    "                colors=[plt.cm.spectral(l / float(n_clusters)), ])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nesse Notebook, iremos discutir algumas abordagens e utilizações de uma rede Neural para realizar uma análise de sentimento de reviews de filmes, de modo automático, em outras palavras iremos \"ensinar\" uma máquina a automaticamente identificar se uma review é positiva ou negativa de um filme. Para isso utilizaremos um dataset preparadado pela [Udacity](https://br.udacity.com/). O dataset está dividido em dois arquivos reviews.txt e reviews_labels.txt, onde respectivamente são as reviews e as classificações de cada um em \"positive\" e \"negative\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada no dataset e dividí-lo adequadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #pre-processamento dos dados\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = open('dados/reviews.txt', 'r') #Abrindo o documento\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sz = 0\n",
    "for i in reviews:\n",
    "    sz += len(i)\n",
    "print(sz/len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = open('dados/reviews_labels.txt', 'r') #Abrindo o documento\n",
    "labels = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, nossa database possui 25 mil instâncias com reviews de em média 1300 palavras, tornando-o suficientemente grande para demorar muito tempo, por tanto vamos trabalhar com um número menor de instâncias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectors = reviews[:2000]\n",
    "train_labels = labels[:2000]\n",
    "test_vectors = reviews[2000:3000]\n",
    "test_labels = labels[2000:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bem, nossos dados ainda precisam de algum pré-processamento para utilizarmos em nossos algoritmos, utilizaremos o método estatístico [TF-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) para darmos pesos para as palavras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_vectors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, utilizando os algoritmos aprensentados até agora, tente classificar as reviews em positivas e negativas, qual algoritmo se apresenta melhor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Acrescente seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O afundamento do RMS Titanic é um dos mais infames naufrágios da história. Em 15 de Abril de 1912, durante sua viagem de estreia, o Titanic naufragou ao colidir com um iceberg, matando 1502 de 2224 passageiros e tribupação.\n",
    "\n",
    "Nesse desafio, pedimos um modelo capaz de analizar que tipos de pessoas tinham mais condições de sobreviver.\n",
    "Tradução livre do Kaggle: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_vectors)\n",
    "test_vectors = vectorizer.transform(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos analizar nossos dados, utilizando a biblioteca Pandas para tirarmos algumas conclusões sobre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dados/train.csv')\n",
    "test = pd.read_csv('dados/test.csv')\n",
    "combine_data = [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os nomes dos descritores de nossa base de dados, precisamos saber o que cada um representa:\n",
    "\n",
    "Survival\tSobrevivência(Target do problema)\t0 = Não, 1 = Sim\n",
    "\n",
    "pclass\tClasse do Ticket (Algo como classes de viagens de avião)\t1 = 1ª 2 = 2ª, 3 = 3ª\n",
    "\n",
    "Sex\tGênero sexual\t\n",
    "\n",
    "Age\tIdade em anos\t\n",
    "\n",
    "Sibsp\t(Siblins) Irmãos ou conjuges\n",
    "\n",
    "Parch\tParentes e filhos\t\n",
    "\n",
    "Ticket\tNúmero do Ticket\t\n",
    "\n",
    "Fare\tTarifa da passagem\t\n",
    "\n",
    "Cabin\tNúmero da cabine\t\n",
    "\n",
    "Embarked\tPorto de embarcação\tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preview the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.describe(include=['O']) # Descrição de atributos categóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Utilizando o que foi aprendido durante o minicurso, tente resolver esse problema com a melhor acurácia possível, perceba que não é necessário fazer o cross_validation, visto que os conjuntos de treinot e teste já estão dividídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Acrescente o código aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
